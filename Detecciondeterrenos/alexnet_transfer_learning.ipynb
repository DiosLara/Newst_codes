{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchsummary import summary\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Transforms to the Data\n",
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        # transforms.RandomResizedCrop(size=300, scale=(0.8, 1.2)),\n",
    "        # transforms.RandomRotation(degrees=45),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        # transforms.RandomVerticalFlip(),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                      [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        # transforms.Resize(size=256),\n",
    "        # transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                      [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                      [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Data\n",
    "\n",
    "# Set train and valid directory paths\n",
    "dataset=r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\class_chinchetas\"\n",
    "\n",
    "train_directory = os.path.join(dataset, 'Train')\n",
    "# dataset = r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\"\n",
    "valid_directory = os.path.join(dataset, 'Validacion')\n",
    "\n",
    "# Batch size\n",
    "bs =100\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(os.listdir(valid_directory))  #10#2#257\n",
    "print(num_classes)\n",
    "\n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'])\n",
    "}\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "print(idx_to_class)\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "device = torch.device(0 if torch.cuda.is_available() else \"cpu\")\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data_loader = DataLoader(data['train'], batch_size=bs, shuffle=False)\n",
    "valid_data_loader = DataLoader(data['valid'], batch_size=bs, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "pd.DataFrame([x.replace(\"\\\\\",\"/\").split(\"/\") for x in glob.glob(train_directory+\"/*/*\")])[6].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size, valid_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## True para entrenar todos los parametro, False para bloaquear ciertos parametros\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### para terrenos y casas\n",
    "# alexnet.features[1]= nn.Hardtanh()\n",
    "# alexnet.classifier[6] = nn.Linear(4096, 4096)\n",
    "# alexnet.classifier.add_module(\"7\",nn.Softplus())\n",
    "# alexnet.classifier.add_module(\"8\", nn.Linear(4096, 4096))\n",
    "# alexnet.classifier.add_module(\"9\",nn.Softplus())\n",
    "# alexnet.classifier.add_module(\"10\", nn.Linear(4096, 2048))\n",
    "# alexnet.classifier.add_module(\"11\", nn.Softplus())\n",
    "# alexnet.classifier.add_module(\"12\", nn.Linear(2048, num_classes))\n",
    "# alexnet.classifier.add_module(\"13\", nn.Softplus())\n",
    "# alexnet.classifier.add_module(\"14\",  nn.LogSoftmax(dim = 1))\n",
    "# alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### para otros modelos\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(alexnet.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "# loss_func = nn.NLLLoss()\n",
    "#loss_func=nn.MSELoss()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.RMSprop(lr=0.0001,params=alexnet.parameters())#alexnet.parameters())\n",
    "optimizer= optim.SGD(alexnet.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_criterion, optimizer,path, mod=0,epochs=25,device=0):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    best_loss=0.0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in tqdm.tqdm(enumerate(train_data_loader),total=len(train_data_loader)):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagate the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            # acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            acc = torch.mean(correct_counts.type(torch.cuda.FloatTensor))\n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "            \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in tqdm.tqdm(enumerate(valid_data_loader),total=len(valid_data_loader)):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "                \n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                # acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "                acc = torch.mean(correct_counts.type(torch.cuda.FloatTensor))\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "            \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size \n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size \n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "                \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "        \n",
    "        # Save if the model has best accuracy till now\n",
    "        #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
    "        if epoch==0:\n",
    "            best_acc=avg_train_acc\n",
    "            best_loss=avg_train_loss\n",
    "            torch.save({    'epoch': epoch,\n",
    "                    'model_state_dict':model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss_func,\n",
    "                    }, path+'/init_mod'+str(mod)+'.pth')\n",
    "        elif epoch!=0 and avg_train_acc>=best_acc and avg_train_loss<=best_loss:\n",
    "            best_acc=avg_train_acc\n",
    "            best_loss=avg_train_loss\n",
    "            torch.save({    'epoch': epoch,\n",
    "                    'model_state_dict':model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss_func,\n",
    "                    }, path+'/best_mod'+str(mod)+'.pth')\n",
    "        torch.save({    'epoch': epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss_func,\n",
    "                }, path+'/last_mod'+str(mod)+'.pth')\n",
    "            \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore_checkpoint\n",
    "checkpoint=torch.load(r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\ckpoint\\5clases\\best_mod11.pth\")\n",
    "alexnet.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(0 if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(0)\n",
    "num_epochs =20\n",
    "\n",
    "\n",
    "trained_model, history = train_and_validate(alexnet.cuda(), loss_func, optimizer,r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\ckpoint/5clases\",11,num_epochs,device=0)\n",
    "\n",
    "#torch.save(history, dataset+'_history.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = np.array(history)\n",
    "plt.plot(history[:,0:2])\n",
    "plt.legend(['Tr Loss', 'Val Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(dataset+'_loss_curve.png')\n",
    "plt.show()\n",
    "plt.plot(history[:,2:4])\n",
    "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(dataset+'_accuracy_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ñ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import rasterio.mask\n",
    "from rasterio.windows import Window\n",
    "import sys\n",
    "from shapely.geometry import mapping\n",
    "sys.path.append(r'E:/gitlab/geoloc2/Detecciondeterrenos')\n",
    "from codigos import Generar_txt\n",
    "###path de yolo dentro de computadora\n",
    "os.chdir(r'C:/Users/ASUS/Inteligencia_Artificial/yolov7')\n",
    "from detect_Alberto_v4 import *\n",
    "from scipy.ndimage import rotate as rotate_image\n",
    "from shapely import geometry\n",
    "import time\n",
    "import datetime\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alexnet1():\n",
    "    def __init__(self,weights,num_classes,idx_to_class):\n",
    "        \"\"\"inicializa el model, con los pesos entrenados\"\"\"\n",
    "        alexnet=models.alexnet(pretrained=True)\n",
    "        self.device = torch.device(0 if torch.cuda.is_available() else \"cpu\")\n",
    "        checkpoint=torch.load(weights,map_location=self.device)\n",
    "        # \n",
    "        alexnet.features[1]= nn.Hardtanh()\n",
    "        alexnet.classifier[6] = nn.Linear(4096, 4096)\n",
    "        alexnet.classifier.add_module(\"7\",nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"8\", nn.Linear(4096, 4096))\n",
    "        alexnet.classifier.add_module(\"9\",nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"10\", nn.Linear(4096, 2048))\n",
    "        alexnet.classifier.add_module(\"11\", nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"12\", nn.Linear(2048, num_classes))\n",
    "        alexnet.classifier.add_module(\"13\", nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"14\",  nn.Softmax(dim = 1))\n",
    "        # for param in alexnet.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        # alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "        alexnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "        summary(alexnet, (3, 224, 224))\n",
    "        self.model=alexnet\n",
    "        self.idx_to_class=idx_to_class\n",
    "    \n",
    "    def predict_file(self,file,pad=True):\n",
    "        \"\"\"Genera prediccion sobre archivo\"\"\"\n",
    "        x = Image.open(file)\n",
    "        x = np.asarray(x)\n",
    "        x=np.stack([x[:,:,0],x[:,:,1],x[:,:,2]], axis=-1)\n",
    "        if pad:\n",
    "            x=padding(x)\n",
    "        x=cv2.resize(x,(224,224))\n",
    "        x=x.astype(\"float32\")\n",
    "        x=x/255*2-1\n",
    "        x=np.moveaxis(x,-1,0)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        with torch.no_grad():\n",
    "            img = torch.from_numpy(x).to(self.device)\n",
    "            res=list(self.model(img).cpu().detach().numpy()[0])\n",
    "            indice=res.index(max(res))\n",
    "            clase=self.idx_to_class.get(indice)\n",
    "        return clase \n",
    "    \n",
    "    def predict_image(self,image,pad=True):\n",
    "        \"\"\"Generar predeccion de clase sobre imagen precargada\"\"\"\n",
    "        x = np.asarray(image)\n",
    "        x=np.stack([x[:,:,0],x[:,:,1],x[:,:,2]], axis=-1)\n",
    "        if pad:\n",
    "            x=padding(x)\n",
    "        imagen=x.copy()\n",
    "        x=cv2.resize(x,(224,224))\n",
    "        x=x.astype(\"float32\")\n",
    "        x=x/255*2-1\n",
    "        x=np.moveaxis(x,-1,0)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        with torch.no_grad():\n",
    "            img = torch.from_numpy(x).to(self.device)\n",
    "            res=list(self.model(img).cpu().detach().numpy()[0])\n",
    "            indice=res.index(max(res))\n",
    "            clase=self.idx_to_class.get(indice)\n",
    "        return clase, imagen \n",
    "    \n",
    "def padding(img):\n",
    "    \"\"\"Escala la imagen y completa el sobrante con franjas negras, para no perder proporciones\"\"\"                \n",
    "    old_image_height, old_image_width, channels = img.shape\n",
    "    new_image_width = 224\n",
    "    new_image_height = 224\n",
    "    color = (0,0,0)\n",
    "    if old_image_height<=old_image_width:\n",
    "        f=new_image_width/old_image_width\n",
    "    else:\n",
    "        f=new_image_height/old_image_height\n",
    "    img=cv2.resize(img,(int(f*old_image_width),int(f*old_image_height)))\n",
    "    old_image_height, old_image_width, channels = img.shape\n",
    "    result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "    x_center = (new_image_width - old_image_width) // 2\n",
    "    y_center = (new_image_height - old_image_height) // 2\n",
    "    try:\n",
    "        result[y_center:y_center+old_image_height, \n",
    "            x_center:x_center+old_image_width] = img\n",
    "    except:\n",
    "        result=img \n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 256, 6, 6]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 64, 55, 55]          23,296\n",
      "|    └─Hardtanh: 2-2                     [-1, 64, 55, 55]          --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 64, 27, 27]          --\n",
      "|    └─Conv2d: 2-4                       [-1, 192, 27, 27]         307,392\n",
      "|    └─ReLU: 2-5                         [-1, 192, 27, 27]         --\n",
      "|    └─MaxPool2d: 2-6                    [-1, 192, 13, 13]         --\n",
      "|    └─Conv2d: 2-7                       [-1, 384, 13, 13]         663,936\n",
      "|    └─ReLU: 2-8                         [-1, 384, 13, 13]         --\n",
      "|    └─Conv2d: 2-9                       [-1, 256, 13, 13]         884,992\n",
      "|    └─ReLU: 2-10                        [-1, 256, 13, 13]         --\n",
      "|    └─Conv2d: 2-11                      [-1, 256, 13, 13]         590,080\n",
      "|    └─ReLU: 2-12                        [-1, 256, 13, 13]         --\n",
      "|    └─MaxPool2d: 2-13                   [-1, 256, 6, 6]           --\n",
      "├─AdaptiveAvgPool2d: 1-2                 [-1, 256, 6, 6]           --\n",
      "├─Sequential: 1-3                        [-1, 5]                   --\n",
      "|    └─Dropout: 2-14                     [-1, 9216]                --\n",
      "|    └─Linear: 2-15                      [-1, 4096]                37,752,832\n",
      "|    └─ReLU: 2-16                        [-1, 4096]                --\n",
      "|    └─Dropout: 2-17                     [-1, 4096]                --\n",
      "|    └─Linear: 2-18                      [-1, 4096]                16,781,312\n",
      "|    └─ReLU: 2-19                        [-1, 4096]                --\n",
      "|    └─Linear: 2-20                      [-1, 4096]                16,781,312\n",
      "|    └─Softplus: 2-21                    [-1, 4096]                --\n",
      "|    └─Linear: 2-22                      [-1, 4096]                16,781,312\n",
      "|    └─Softplus: 2-23                    [-1, 4096]                --\n",
      "|    └─Linear: 2-24                      [-1, 2048]                8,390,656\n",
      "|    └─Softplus: 2-25                    [-1, 2048]                --\n",
      "|    └─Linear: 2-26                      [-1, 5]                   10,245\n",
      "|    └─Softplus: 2-27                    [-1, 5]                   --\n",
      "|    └─Softmax: 2-28                     [-1, 5]                   --\n",
      "==========================================================================================\n",
      "Total params: 98,967,365\n",
      "Trainable params: 98,967,365\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 850.99\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 3.84\n",
      "Params size (MB): 377.53\n",
      "Estimated Total Size (MB): 381.95\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# weights=r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\ckpoint/5clases\\init_mod0.pth\"\n",
    "# weights=r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\ckpoint/5clases\\best_mod0.pth\"\n",
    "weights=r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\ckpoint\\5clases\\best_modconstrucciones_2.pth\"\n",
    "num_classes=5\n",
    "diciconario={0: 'casas', 1: 'en_construccion', 2: 'establecimiento', 3: 'multivivienda', 4: 'terreno_baldio'}\n",
    "model_class=alexnet1(weights=weights,num_classes=num_classes,idx_to_class=diciconario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27256/27256 [03:59<00:00, 114.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([array([    0.54521,   0.0052463,    0.086892,    0.016067,     0.34659]),\n",
       "  array([    0.19126,     0.62842,    0.021858,   0.0054645,     0.15301]),\n",
       "  array([    0.13343,   0.0016088,     0.41582,    0.005962,     0.44317]),\n",
       "  array([    0.64907,   0.0077984,    0.054589,    0.056989,     0.23155]),\n",
       "  array([   0.022348,   0.0022727,    0.012121,   0.0011364,     0.96212])],\n",
       " ['casas',\n",
       "  'en_construccion',\n",
       "  'establecimiento',\n",
       "  'multivivienda',\n",
       "  'terreno_baldio'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "y_true = []\n",
    "y_predict =[]\n",
    "import glob\n",
    "filenames=glob.glob(r\"D:/alexnet\\train_pad/*/*\")\n",
    "for file in tqdm.tqdm(filenames):\n",
    "    file=file.replace(\"\\\\\",\"/\")\n",
    "    if file[-3:]==\"ini\":\n",
    "        continue\n",
    "    y_true.append(file.split(\"/\")[-2])\n",
    "    y_predict.append(model_class.predict_file(file,pad=True))\n",
    "labels=[]\n",
    "for k,v in diciconario.items():\n",
    "    labels.append(v)\n",
    "labels\n",
    "confusion_mat = confusion_matrix(y_true, y_predict,labels=labels)\n",
    "conteos=pd.DataFrame(y_true,columns=[\"clases\"]).value_counts()\n",
    "conteos=conteos.reset_index(drop=False)\n",
    "matrix=[]\n",
    "for i,label in enumerate(labels):\n",
    "    matrix.append(confusion_mat[i]/conteos[conteos[\"clases\"]==label][0].values[0])\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(matrix)\n",
    "matrix,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJjUlEQVR4nO3d3YtchR3G8efJZo2xvhHMhWRDI0VsU6FK11QIlJJKG1/QXirolZAbhQiC6EUv/AfEG6GkKhYURdALsZYQUBFBo6tGSbpaUvElKEQJovEtbvL0YreQ1t3smck5c3Z+fD+wsJNZzjyE+e6ZnV1mnEQA6ljV9wAA7SJqoBiiBoohaqAYogaKWd3FQdetW5UNUxNdHLp1H+4/p+8JA3LfAwYzRr9dscfn//bbfK1j+W7RwZ1EvWFqQk///YIuDt262372u74nDGZiPL5Z/leOHet7QmOr1qzpe0Jjr3733JLX8fAbKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBooplHUtrfbfs/2Qdt3dz0KwPCWjdr2hKQHJF0tabOkm2xv7noYgOE0OVNvkXQwyftJjkl6QtIN3c4CMKwmUW+Q9PFJlw8t/Nv/sL3D9oztmSNHTrS1D8CAmkS92MuQ/uh1X5PsSjKdZHrdOp5/A/rSpL5DkjaedHlK0ifdzAFwuppE/bqki21fZPsMSTdKeqbbWQCGteyL+SeZs327pN2SJiQ9nORA58sADKXRO3QkeU7S0m8JAGDF4BktoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKafQiCYP68N/rddufdnRx6Nb9cu9s3xMGsv+KH73m48qW8dm76vzz+p7Q3OcTS17FmRoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGihm2ahtP2z7sO39oxgE4PQ0OVM/Iml7xzsAtGTZqJO8JOnICLYAaAE/UwPFtBa17R22Z2zP/DD3TVuHBTCg1qJOsivJdJLpydVntXVYAAPi4TdQTJNfaT0u6RVJl9g+ZPvW7mcBGNay79CR5KZRDAHQDh5+A8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQzLIvkjCMH86e0Ke/Pa+LQ7cuvz7R94SB/OuvV/Q9YSA/33mg7wmN5fgY3Rey9FWcqYFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGihm2ahtb7T9gu1Z2wds7xzFMADDafIaZXOS7kzypu1zJL1he0+Sf3a8DcAQlj1TJ/k0yZsLn38laVbShq6HARjOQD9T294k6XJJexe5boftGdszx7/9uqV5AAbVOGrbZ0t6StIdSb78/+uT7EoynWR6Yu1P2twIYACNorY9qfmgH0vydLeTAJyOJs9+W9JDkmaT3Nf9JACno8mZequkWyRts71v4eOajncBGNKyv9JK8rIkj2ALgBbwF2VAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRTT5HW/B7bq3Dmt+cNnXRy6df7Lmr4nDGTznz/qe8JAjm67tO8Jja3dva/vCY3l+PElr+NMDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFLNs1LbPtP2a7bdtH7B97yiGARhOk5cz+l7StiRHbU9Ketn2P5K82vE2AENYNuokkXR04eLkwke6HAVgeI1+prY9YXufpMOS9iTZ2+kqAENrFHWS40kukzQlaYvtH71EpO0dtmdsz8x9+U3LMwE0NdCz30m+kPSipO2LXLcryXSS6dXnntXOOgADa/Ls93rb5y98vlbSVZLe7XgXgCE1efb7Qkl/sz2h+W8CTyZ5tttZAIbV5NnvdyRdPoItAFrAX5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVCM518BuF3nel1+49+3flxIsvteMJgO7l9d2f3Jvr4nNLbljx9r5u3vFr0zcKYGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgmMZR256w/ZbtZ7scBOD0DHKm3ilptqshANrRKGrbU5KulfRgt3MAnK6mZ+r7Jd0l6cRSX2B7h+0Z2zM/6Ps2tgEYwrJR275O0uEkb5zq65LsSjKdZHpSa1obCGAwTc7UWyVdb/sDSU9I2mb70U5XARjaslEnuSfJVJJNkm6U9HySmztfBmAo/J4aKGb1IF+c5EVJL3ayBEArOFMDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVCMk7R/UPszSR+2fNgLJH3e8jG7NE57x2mrNF57u9r60yTrF7uik6i7YHsmyXTfO5oap73jtFUar719bOXhN1AMUQPFjFPUu/oeMKBx2jtOW6Xx2jvyrWPzMzWAZsbpTA2gAaIGihmLqG1vt/2e7YO27+57z6nYftj2Ydv7+96yHNsbbb9ge9b2Ads7+960FNtn2n7N9tsLW+/te1MTtidsv2X72VHd5oqP2vaEpAckXS1ps6SbbG/ud9UpPSJpe98jGpqTdGeSX0i6UtJtK/j/9ntJ25L8StJlkrbbvrLfSY3slDQ7yhtc8VFL2iLpYJL3kxzT/Dtv3tDzpiUleUnSkb53NJHk0yRvLnz+lebvfBv6XbW4zDu6cHFy4WNFP8tre0rStZIeHOXtjkPUGyR9fNLlQ1qhd7xxZnuTpMsl7e15ypIWHsruk3RY0p4kK3brgvsl3SXpxChvdByi9iL/tqK/Q48b22dLekrSHUm+7HvPUpIcT3KZpClJW2xf2vOkJdm+TtLhJG+M+rbHIepDkjaedHlK0ic9bSnH9qTmg34sydN972kiyReaf/fVlfzcxVZJ19v+QPM/Mm6z/egobngcon5d0sW2L7J9hubf+P6ZnjeVYNuSHpI0m+S+vveciu31ts9f+HytpKskvdvrqFNIck+SqSSbNH+ffT7JzaO47RUfdZI5SbdL2q35J3KeTHKg31VLs/24pFckXWL7kO1b+950Clsl3aL5s8i+hY9r+h61hAslvWD7Hc1/o9+TZGS/Jhon/JkoUMyKP1MDGAxRA8UQNVAMUQPFEDVQDFEDxRA1UMx/ANtNCfK84rwfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "establecimiento\n",
      "establecimiento\n"
     ]
    }
   ],
   "source": [
    "## test clasificador sobre archivo\n",
    "file=r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\validacion_pad\\casas\\0_10.png\"\n",
    "print(model_class.predict_file(file,pad=True))\n",
    "##test clasificador sobre imagen precargada\n",
    "image=cv2.imread(file)\n",
    "print(model_class.predict_image(image,pad=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
