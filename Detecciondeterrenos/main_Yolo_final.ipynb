{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No borrar path compañeros\n",
    "path_append_hector = '/home/hector/Documentos/Infis/Geo/Data/respaldo codigos/Detecciondeterrenos'\n",
    "path_chdir_hector = '/home/hector/Documentos/Infis/Geo/yolov7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hector/anaconda3/envs/Infis/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.11.1-CAPI-1.17.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_14071/2587678701.py:5: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import rasterio.mask\n",
    "from rasterio.windows import Window\n",
    "import sys\n",
    "from shapely.geometry import mapping\n",
    "sys.path.append(path_append_hector)\n",
    "from codigos import Generar_txt\n",
    "###path de yolo dentro de computadora\n",
    "os.chdir(path_chdir_hector)\n",
    "from detect_Alberto_v4 import *\n",
    "from scipy.ndimage import rotate as rotate_image\n",
    "from shapely import geometry\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de deteccion de objetos basado en yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# No borrar paths de compañeros\n",
    "pesos_alberto = r'C:\\Users\\ASUS\\Inteligencia_Artificial\\yolov7\\runs\\train\\Alberto_a22\\weights\\best_Alberto_a22.pt'\n",
    "pesos_hector = 'Modelos/Chinchetas_best_ruben_1.pt'\n",
    "\n",
    "# Poner tus pesos\n",
    "Modelo=modelo(weights=pesos_hector)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo clasificador basado en Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alexnet1():\n",
    "    def __init__(self,weights,num_classes,idx_to_class):\n",
    "        \"\"\"inicializa el model, con los pesos entrenados\"\"\"\n",
    "        alexnet=models.alexnet(pretrained=True)\n",
    "        self.device = torch.device(0 if torch.cuda.is_available() else \"cpu\")\n",
    "        checkpoint=torch.load(weights,map_location=self.device)\n",
    "#         alexnet.features[1]= nn.Hardtanh()\n",
    "#         alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "#         alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "        # alexnet\n",
    "        alexnet.features[1]= nn.Hardtanh()\n",
    "        alexnet.classifier[6] = nn.Linear(4096, 4096)\n",
    "        alexnet.classifier.add_module(\"7\",nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"8\", nn.Linear(4096, 4096))\n",
    "        alexnet.classifier.add_module(\"9\",nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"10\", nn.Linear(4096, 2048))\n",
    "        alexnet.classifier.add_module(\"11\", nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"12\", nn.Linear(2048, num_classes))\n",
    "        alexnet.classifier.add_module(\"13\", nn.Softplus())\n",
    "        alexnet.classifier.add_module(\"14\",  nn.LogSoftmax(dim = 1))\n",
    "        # for param in alexnet.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "        # alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "        alexnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "        summary(alexnet, (3, 224, 224))\n",
    "        self.model=alexnet\n",
    "        self.idx_to_class=idx_to_class\n",
    "    \n",
    "    def predict_file(self,file,pad=True):\n",
    "        \"\"\"Genera prediccion sobre archivo\"\"\"\n",
    "#         x = Image.open(file)\n",
    "#         x = np.asarray(x)\n",
    "#         x=np.stack([x[:,:,0],x[:,:,1],x[:,:,2]], axis=-1)\n",
    "        x=cv2.imread(file)\n",
    "        if pad:\n",
    "            x=padding(x)\n",
    "        x=cv2.resize(x,(224,224))\n",
    "        x=x.astype(\"float32\")\n",
    "        x=x/255*2-1\n",
    "        x=np.moveaxis(x,-1,0)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        with torch.no_grad():\n",
    "            img = torch.from_numpy(x).to(self.device)\n",
    "            res=list(self.model(img).cpu().detach().numpy()[0])\n",
    "            indice=res.index(max(res))\n",
    "            clase=self.idx_to_class.get(indice)\n",
    "        return clase \n",
    "    \n",
    "    def predict_image(self,image,pad=True):\n",
    "        \"\"\"Generar predeccion de clase sobre imagen precargada\"\"\"\n",
    "#         x = np.asarray(image)\n",
    "#         x=np.stack([x[:,:,2],x[:,:,1],x[:,:,2]], axis=-1)\n",
    "        x=np.array(image)\n",
    "        if pad:\n",
    "            x=padding(x)\n",
    "        imagen=x.copy()\n",
    "        x=cv2.resize(x,(224,224))\n",
    "        x=x.astype(\"float32\")\n",
    "        x=x/255*2-1\n",
    "        x=np.moveaxis(x,-1,0)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        with torch.no_grad():\n",
    "            img = torch.from_numpy(x).to(self.device)\n",
    "            res=list(self.model(img).cpu().detach().numpy()[0])\n",
    "            indice=res.index(max(res))\n",
    "            clase=self.idx_to_class.get(indice)\n",
    "        return clase, imagen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\ckpoint\\5clases\\best_modconstrucciones_2.pth\"\n",
    "num_classes=5\n",
    "diciconario={0: 'casas', 1: 'en_construccion', 2: 'establecimiento', 3: 'multivivienda', 4: 'terreno_baldio'}\n",
    "model_class=alexnet1(weights=weights,num_classes=num_classes,idx_to_class=diciconario)\n",
    "# num_classes=6\n",
    "# diciconario={0: 'carros', 1: 'casas', 2: 'en_construccion', 3: 'establecimiento', 4: 'multivivienda', 5: 'terreno_baldio'}\n",
    "# model_class=alexnet(weights=weights,num_classes=num_classes,idx_to_class=diciconario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test clasificador sobre archivo\n",
    "# file=r\"D:\\alexnet\\train_pad\\en_construccion\\Copia de Huixqui_1.PNG\"\n",
    "# print(model_class.predict_file(file,pad=True))\n",
    "# ##test clasificador sobre imagen precargada\n",
    "# image=cv2.imread(file)\n",
    "# print(model_class.predict_image(image,pad=True)[0])\n",
    "# cv2_imshow(\"a\",image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch #or pytorch, works for the same shit and contains the same libraries that you need such as Numpy (as far as I know)\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# y_true = []\n",
    "# y_predict =[]\n",
    "# import glob\n",
    "# filenames=glob.glob(r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\train_pad/*/*\")\n",
    "# for file in tqdm.tqdm(filenames):\n",
    "#     file=file.replace(\"\\\\\",\"/\")\n",
    "#     y_true.append(file.split(\"/\")[-2])\n",
    "#     y_predict.append(model_class.predict_file(file,pad=False))\n",
    "# labels=[]\n",
    "# for k,v in diciconario.items():\n",
    "#     labels.append(v)\n",
    "# labels\n",
    "# confusion_mat = confusion_matrix(y_true, y_predict,labels=labels)\n",
    "# conteos=pd.DataFrame(y_true,columns=[\"clases\"]).value_counts()\n",
    "# conteos=conteos.reset_index(drop=False)\n",
    "# matrix=[]\n",
    "# for i,label in enumerate(labels):\n",
    "#     matrix.append(confusion_mat[i]/conteos[conteos[\"clases\"]==label][0].values[0])\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(matrix)\n",
    "# matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros del raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: internal_proj_identify: /home/hector/anaconda3/envs/Infis/share/proj/proj.db lacks DATABASE.LAYOUT.VERSION.MAJOR / DATABASE.LAYOUT.VERSION.MINOR metadata. It comes from another PROJ installation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(245,\n",
       " 210,\n",
       " 251,\n",
       " CRS.from_wkt('LOCAL_CS[\"WGS 84 / Pseudo-Mercator\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]'),\n",
       " 61629,\n",
       " 52787,\n",
       " -11084278.6993,\n",
       " -11063163.9539,\n",
       " 2181047.5457,\n",
       " 2205699.1587)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No borrar path de compañeros\n",
    "raster_hector = r\"/home/hector/Descargas/temporal/Chinchetas_atlacomulco/Lerma_chinchetas.tif\"\n",
    "raster_alberto=r\"D:\\otros\\MONCLOVA\\a2.tif\"\n",
    "\n",
    "# Cambiar al tuyo\n",
    "raster = raster_hector\n",
    "\n",
    "alto,ancho,dim,crs,H,W,minx,maxx,miny,maxy=Parametro_raster(raster,metros=100)\n",
    "alto,ancho,dim,crs,H,W,minx,maxx,miny,maxy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delimitacion por shape de municipio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No borrar path compañeros\n",
    "path_mz_alberto = r\"C:\\Users\\ASUS\\Documents\\GobiernoEdomex\\Agua\\geoshapes\\Full_manzanas\\Manzana_Atlacomulco.shp\"\n",
    "path_mz_hector = '/home/hector/Documentos/Infis/Geo/Data/Shapes/Lerma/LERMA/Zona_Lerma.shp'\n",
    "\n",
    "# CAMBIAR AL TUYO\n",
    "path_mz = path_mz_hector\n",
    "\n",
    "shape_o=gpd.read_file(path_mz)\n",
    "shape_o=shape_o.to_crs('3857')\n",
    "shape=shape_transform(shape_o.copy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar prediccion sobre raster(municipio) con salida en shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres=[]\n",
    "imshow=False\n",
    "result=pd.DataFrame()\n",
    "casas=[]\n",
    "terreno=[]\n",
    "angulosget=[]\n",
    "conf_casas=[]\n",
    "conf_terreno=[]\n",
    "clase_casas=[]\n",
    "clase_terreno=[]\n",
    "s=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impementar modelos combinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postproceso(Modelo,model_class,casas,conf_casas,clase_casas,terreno,conf_terreno,clase_terreno,\n",
    "            raster,ancho,alto,dim,minx,maxx,miny,maxy,shape,angulo_get=0,opt_conf_thres=0.2,imshow=False,imsave=False,path=\"C:/Users/ASUS/salida/imagen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_casas=gpd.GeoDataFrame({\"clase_detectada\":clase_casas,\"conf\":conf_casas},geometry=casas,crs=crs)\n",
    "gdf_casas.set_crs=crs\n",
    "gdf_casas[\"area\"]=gdf_casas.area\n",
    "gdf_casas=gdf_casas.astype({\"conf\":\"float64\"})\n",
    "gdf_casas.to_file(r\"C:/Users/ASUS/salida/shape/ortofoto_casas_nuevomodeloa22_mod4_5_80.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_terreno=gpd.GeoDataFrame({\"clase_detectada\":clase_terreno,\"conf\":conf_terreno},geometry=terreno,crs=crs)\n",
    "gdf_terreno.set_crs=crs\n",
    "gdf_terreno[\"area\"]=gdf_terreno.area\n",
    "gdf_terreno=gdf_terreno.astype({\"conf\":\"float64\"})\n",
    "gdf_terreno.to_file(r\"C:/Users/ASUS/salida/shape/ortofoto_terrenos_nuevomodeloa22_mod4_5_80.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint=torch.load(r'C:\\Users\\ASUS\\Inteligencia_Artificial\\clasificador\\ckpoint/best_mod11.pth',map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ñ ##breakpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yolo por manzanas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampliar_shape(shape,factor_ampliacion=2):\n",
    "    \"\"\"Amplifica el polygon de cada manzana con el fin de extrar imagenes sin perder informacion de la manzana\"\"\"\n",
    "    shape[\"geometry\"]=shape[\"geometry\"].envelope\n",
    "    shape['centroid']=shape.centroid\n",
    "    geometry=[]\n",
    "    clase=[]\n",
    "    for i,polygon in enumerate(shape['geometry']):\n",
    "        try:\n",
    "            point=mapping(shape['centroid'][i]).get('coordinates')\n",
    "        except:\n",
    "            continue\n",
    "        x=point[0]\n",
    "        y=point[1]\n",
    "        go=[]\n",
    "        #print(x)\n",
    "        coodinates=mapping(polygon).get('coordinates')[0]\n",
    "        for a in coodinates:\n",
    "            x1=a[0]\n",
    "            y1=a[1]\n",
    "            x2=x+(x1-x)*factor_ampliacion\n",
    "            y2=y+(y1-y)*factor_ampliacion\n",
    "            go.append((x2,y2))\n",
    "        geometry.append(Polygon(go))\n",
    "        #clase.append(shape.loc[i,\"clase_dete\"])\n",
    "    return gpd.GeoDataFrame(geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lerma'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_iteracion = 'zona'\n",
    "\n",
    "nombre_manzana = path_mz.replace('\\\\','/').split('/')[-1].split('.')[0]\n",
    "nombre_mun = nombre_manzana.split('_')[-1]\n",
    "nombre_mun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zona  =  48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "col_iteracion = 'zona'\n",
    "\n",
    "nombre_manzana = path_mz.replace('\\\\','/').split('/')[-1].split('.')[0]\n",
    "nombre_mun = nombre_manzana.split('_')[-1]\n",
    "save_tif_cache = r'/home/hector/Descargas/temporal/'+nombre_manzana+'.tif'\n",
    "save_file = '/home/hector/Documentos/Infis/Geo/Data/Shapes/Lerma/Chinchetas'\n",
    "mode_save_file = 'multi'  ## multi, unico\n",
    "is_chinchetas = True\n",
    "\n",
    "\n",
    "with rasterio.open(raster) as scr:\n",
    "    for s in tqdm.tqdm(shape_o[col_iteracion].unique()):\n",
    "        print(col_iteracion, ' = ', s)\n",
    "        \n",
    "        # Inicializamos listas para el  post-proceso\n",
    "        nombres=[]\n",
    "        result=pd.DataFrame()\n",
    "        casas=[]\n",
    "        terreno=[]\n",
    "        angulosget=[]\n",
    "        conf_casas=[]\n",
    "        conf_terreno=[]\n",
    "        clase_casas=[]\n",
    "        clase_terreno=[]\n",
    "        \n",
    "        # Tomamos el filtro n-esimo\n",
    "        shape_aux = shape_o.loc[shape_o[col_iteracion]==s]\n",
    "        \n",
    "        \n",
    "        # Hacemos el corte por filtro\n",
    "        manzana,out_transform = rasterio.mask.mask(scr, ampliar_shape(shape_aux.copy(),1).geometry.values,\n",
    "                                                   crop=True)\n",
    "        out_meta = scr.meta\n",
    "        # Configuracion del guardado del tif\n",
    "        out_meta.update({'driver': 'GTiff',\n",
    "        'height': manzana.shape[1],\n",
    "        'width': manzana.shape[2],\n",
    "        'transform': out_transform})\n",
    "        \n",
    "        # Generamos tif\n",
    "        output_file = save_tif_cache\n",
    "        with rasterio.open(output_file, 'w', **out_meta) as dest:\n",
    "            dest.write(manzana)\n",
    "            \n",
    "        #cve_cat=shape[\"cve_cat\"][s]\n",
    "        #print(cve_cat)\n",
    "        \n",
    "        # Obtenemos parametros del raster\n",
    "        alto,ancho,dim,crs,H,W,minx,maxx,miny,maxy=Parametro_raster(output_file,metros=80)\n",
    "        \n",
    "        # Aplicamos deteccion\n",
    "        postproceso(Modelo,'model_class',casas,conf_casas,\n",
    "                clase_casas,terreno,conf_terreno,clase_terreno,\n",
    "                raster,ancho,alto,dim,minx,maxx,miny,maxy,shape_transform(shape_aux.copy()),angulo_get=0.00000001,\n",
    "                opt_conf_thres=0.2,imshow=False,imsave=False,path=\"C:/Users/ASUS/salida/imagen\",\n",
    "                clasificar=False, cutline_factor=2)\n",
    "        \n",
    "        # Convertimos GeoDataFrame la deteccion\n",
    "        #polygon=shape_o[shape_o['cve_cat']==cve_cat][\"geometry\"].values[0]\n",
    "        gdf_casas=gpd.GeoDataFrame({'conf':conf_casas},geometry=casas,crs=scr.crs)\n",
    "        gdf_casas.set_crs=scr.crs\n",
    "        gdf_casas['area']=gdf_casas.area\n",
    "        gdf_casas=gdf_casas.astype({'conf':'float64'})\n",
    "        #gdf_casas[\"cve_cat\"]=cve_cat\n",
    "        #indices=[]\n",
    "        #for i,point in enumerate(gdf_casas[\"geometry\"].centroid):\n",
    "        #    if polygon.contains(point)==True:\n",
    "        #        indices.append(i)\n",
    "        #gdf_casas=gdf_casas.iloc[indices].reset_index(drop=True)\n",
    "        #gdf_casas[\"centroid_x\"]=[int(x/18) for x in gdf_casas.centroid.x]\n",
    "        #gdf_casas[\"centroid_y\"]=[int(y/18) for y in gdf_casas.centroid.y]\n",
    "        #gdf_casas=gdf_casas.sort_values(by=\"conf\",ascending=False).drop_duplicates([\"centroid_x\",\"centroid_y\"],keep=\"first\")\n",
    "        \n",
    "        # Guardamos\n",
    "        if is_chinchetas:\n",
    "            nombre_mun = 'Chinchetas_'+nombre_mun\n",
    "        else:\n",
    "            nombre_mun = 'New_casas_'+nombre_mun\n",
    "            \n",
    "        if mode_save_file == 'multi':\n",
    "            gdf_casas.to_file(save_file+'/'+nombre_mun+'_'+col_iteracion+'_'+s+'.shp', index=False)\n",
    "            \n",
    "        else:\n",
    "            gdf_casas[col_iteracion] = s\n",
    "            try:\n",
    "                gdf_casas.to_file(save_file+'/'+nombre_mun+'.shp',mode=\"a\")\n",
    "            except:\n",
    "                gdf_casas.to_file(save_file+'/'+nombre_mun+'.shp')\n",
    "                \n",
    "        if is_chinchetas:\n",
    "            continue     \n",
    "        else:\n",
    "            nombre_mun = 'New_terreno_'+nombre_mun\n",
    "        gdf_terreno=gpd.GeoDataFrame({'conf':conf_terreno},geometry=terreno,crs=scr.crs)\n",
    "        gdf_terreno.set_crs=scr.crs\n",
    "        #indices=[]\n",
    "        #for i,point in enumerate(gdf_terreno[\"geometry\"].centroid):\n",
    "        #    if polygon.contains(point)==True:\n",
    "        #        indices.append(i)\n",
    "        #gdf_terreno=gdf_terreno.iloc[indices].reset_index(drop=True)\n",
    "        #gdf_terreno[\"centroid_x\"]=[int(x/5) for x in gdf_terreno.centroid.x]\n",
    "        #gdf_terreno[\"centroid_y\"]=[int(y/5) for y in gdf_terreno.centroid.y]\n",
    "        #gdf_terreno=gdf_terreno.sort_values(by=\"conf\",ascending=False).drop_duplicates([\"centroid_x\",\"centroid_y\"],keep=\"first\")\n",
    "        if mode_save_file == 'multi':\n",
    "            gdf_terreno.to_file(save_file+'/'+nombre_mun+'_'+col_iteracion+'_'+s+'.shp', index=False)\n",
    "        else:\n",
    "            gdf_terreno[col_iteracion] = s\n",
    "            try:\n",
    "                gdf_terreno.to_file(save_file+'/'+nombre_mun+'.shp',mode=\"a\")\n",
    "            except:\n",
    "                gdf_terreno.to_file(save_file+'/'+nombre_mun+'.shp')\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar imagenes mosaico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path donde se guardan images\n",
    "path_mosaico_salida=r\"D:\\instream\\Mi unidad\\geoshaps\\neza\\images/\"\n",
    "generar_imagenes=int(input(\"Generar imagenes: 1 si, 0 no \"))\n",
    "if generar_imagenes==1:\n",
    "    generar_imagenes_sinrotar=int(input(\"Generar imagenes sin rotar: 1 si, 0 no \"))\n",
    "    generar_imagenes_rotadas=int(input(\"Generar imagenes rotadas: 1 si, 0 no \"))\n",
    "    with tqdm.tqdm(total=alto*ancho) as pbar:\n",
    "        for j in range(0,ancho):#ancho\n",
    "            for i in (range(alto)):#alto\n",
    "                generar=0\n",
    "                label=raster.replace(\"\\\\\",\"/\").split(\"/\")[-1][:-4]+\"_\"\n",
    "                nameimg=label.lower()+str(i)+\"_\"+str(j)\n",
    "                cuadro=[]\n",
    "                for k in range(2):\n",
    "                    for l in range(2):\n",
    "                        cuadro.append((minx+(maxx-minx)/ancho*(j+k),\n",
    "                                       maxy-(maxy-miny)/alto*(i+l),\n",
    "                                       0.0))\n",
    "                cuadro=[cuadro[0],cuadro[1],cuadro[3],cuadro[2],cuadro[0]]\n",
    "                for punto in cuadro:\n",
    "                    x=float(punto[0])\n",
    "                    y=float(punto[1])\n",
    "                    if len(shape[(shape[0]<=x)&(shape[2]>=x)&(shape[1]<=y)&(shape[3]>=y)])>0:\n",
    "                        generar=1             \n",
    "                if generar==1:\n",
    "                    shapes=[{\"type\":'Polygon','coordinates':[cuadro]}]\n",
    "                    array, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "                    four_images=[array[2],array[1],array[0]]\n",
    "                    imagen_n = np.stack(four_images, axis=-1)\n",
    "                    if generar_imagenes_sinrotar==1:\n",
    "                        cv2.imwrite(path_mosaico_salida+nameimg+\".png\",imagen_n)\n",
    "                    if generar_imagenes_rotadas==1:\n",
    "                        angulo_1,imagen_ro=correct_orientation(imagen_n,dim)\n",
    "                        cv2.imwrite(path_mosaico_salida+nameimg+\"_\"+str(angulo_1)+\".png\",imagen_ro)\n",
    "                pbar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar prediccion sobre imagen con salida txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mosaico_salida=r\"C:\\Users\\ASUS\\Inteligencia_Artificial\\yolov7\\train\\imagess\"\n",
    "vector=Modelo.detect(opt_source=path_mosaico_salida,opt_conf_thres=0.3)\n",
    "Generar_txt(vector,path_mosaico_salida+\"labels\")\n",
    "with open(path_mosaico_salida+\"labels/classes.txt\",\"w\") as f:\n",
    "    f.writelines(\"\\n\".join([\"casa\",\"terreno_baldio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "filenames=glob.glob(\"train/images/*\")\n",
    "# for file in filenames:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm.tqdm(range(3476,len(filenames))):\n",
    "    file=filenames[i]\n",
    "    imagen_n=cv2.imread(file)\n",
    "    # assert imagen_n is not None,  \"file could not be read, check with os.path.exists()\"\n",
    "    try:\n",
    "        imagen_n=cv2.resize(imagen_n,(256,256))\n",
    "    except:\n",
    "        continue\n",
    "    dim=imagen_n.shape[0]\n",
    "    angulo=correct_orientation(imagen_n,dim=dim)[0]\n",
    "    image_ro=imagen_n.copy()\n",
    "    image_ro=rotate_image(image_ro,angulo,reshape=True)\n",
    "    image_ro=cv2.resize(image_ro,(256,256))\n",
    "    cv2.imwrite(file.replace(\"images\",\"imagess\"),image_ro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shape igecem de manzanas por municipio (solo pora tener la delimitacion geografica del municipio)\n",
    "shape=gpd.read_file(r\"C:\\Users\\ASUS\\Documents\\GobiernoEdomex\\Agua\\geoshapes\\Full manzanas\\Manzana_Naucalpan.shp\")\n",
    "shape=shape.to_crs(\"3857\")\n",
    "c=[]\n",
    "for manzana in range(len(shape)):\n",
    "    proyecciones1=mapping(shape[\"geometry\"][manzana]).get(\"coordinates\")\n",
    "    if len(proyecciones1)>1:\n",
    "        for manzan in proyecciones1:\n",
    "            print(manzana)\n",
    "            try:\n",
    "                proyecciones=manzan\n",
    "            except:\n",
    "                proyecciones=manzan[0]\n",
    "            point1=np.min((proyecciones,proyecciones),axis=1)[0]\n",
    "            min_y,min_x=point1[0],point1[1]\n",
    "            point2=np.max((proyecciones,proyecciones),axis=1)[0]\n",
    "            max_y,max_x=point2[0],point2[1]\n",
    "            c.append(\",\".join([str(min_y),str(min_x),str(max_y),str(max_x)]))#,proyecciones\n",
    "    else:\n",
    "        proyecciones=proyecciones1[0]\n",
    "        point1=np.min((proyecciones,proyecciones),axis=1)[0]\n",
    "        min_y,min_x=point1[0],point1[1]\n",
    "        point2=np.max((proyecciones,proyecciones),axis=1)[0]\n",
    "        max_y,max_x=point2[0],point2[1]\n",
    "        c.append(\",\".join([str(min_y),str(min_x),str(max_y),str(max_x)]))#,proyecciones    \n",
    "    \n",
    "    # point1=np.min((proyecciones,proyecciones),axis=1)[0]\n",
    "    # min_y,min_x=point1[0],point1[1]\n",
    "    # point2=np.max((proyecciones,proyecciones),axis=1)[0]\n",
    "    # max_y,max_x=point2[0],point2[1]\n",
    "    # c.append(\",\".join([str(min_y),str(min_x),str(max_y),str(max_x)]))#,proyecciones\n",
    "shape1=pd.DataFrame()\n",
    "shape1[\"points\"]=c\n",
    "shape1=shape1[\"points\"].str.split(\",\",expand=True)\n",
    "shape1=shape1.astype({0:\"float64\",1:\"float64\",2:\"float64\",3:\"float64\"})\n",
    "shape=shape1\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "geo=[]\n",
    "shape=gpd.read_file(r\"C:\\Users\\ASUS\\Documents\\GobiernoEdomex\\Agua\\geoshapes\\Full manzanas\\Manzana_Naucalpan.shp\")\n",
    "shape=shape.to_crs(\"3857\")\n",
    "c=[]\n",
    "for manzana in range(len(shape)):\n",
    "    proyecciones1=mapping(shape[\"geometry\"][manzana]).get(\"coordinates\")\n",
    "    for i in proyecciones1[0]:\n",
    "        geo.append(Point(i))\n",
    "sa=gpd.GeoDataFrame(geometry=geo)\n",
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.to_file(r\"C:\\Users\\ASUS\\Desktop\\nau\\pointnau.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.sjoin(shape,shape1 , how='left', op='intersects').to_file(r\"C:\\Users\\ASUS\\Desktop\\nau\\sol.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1=gpd.read_file(r\"C:\\Users\\ASUS\\Desktop\\nau\\new_terreno1.shp\")z\n",
    "shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=[]\n",
    "for i,pol in enumerate(shape1[\"geometry\"]):\n",
    "    generar=0\n",
    "    cuadro=mapping(pol).get(\"coordinates\")[0]\n",
    "    for punto in cuadro:\n",
    "            x=float(punto[0])\n",
    "            y=float(punto[1])\n",
    "            if len(shape[(shape[0]<=x)&(shape[2]>=x)&(shape[1]<=y)&(shape[3]>=y)])>0:\n",
    "                generar=1             \n",
    "    if generar==1:\n",
    "         indices.append(i)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1.loc[indices].to_file(r\"C:\\Users\\ASUS\\Desktop\\nau/solo_naucalpan.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.sjoin(shape,shape1 , how='inner', op='intersects').to_file(r\"C:\\Users\\ASUS\\Desktop\\nau/sol.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_polys = [polygon for polygon in  if (polygon.centroid.x**2 + polygon.centroid.y**2 < 4**2)]\n",
    "\n",
    "complete_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "with rasterio.open(r\"D:\\haar\\classes\\carros.tif\") as src:\n",
    "\n",
    "    # The size in pixels of your desired window\n",
    "    xsize, ysize = 1000,1000\n",
    "\n",
    "    # Generate a random window origin (upper left) that ensures the window \n",
    "    # doesn't go outside the image. i.e. origin can only be between \n",
    "    # 0 and image width or height less the window width or height\n",
    "    xmin, xmax = 0, src.width - xsize\n",
    "    ymin, ymax = 0, src.height - ysize\n",
    "    xoff, yoff = random.randint(xmin, xmax), random.randint(ymin, ymax)\n",
    "\n",
    "    # Create a Window and calculate the transform from the source dataset    \n",
    "    window = Window(0, 0, xsize, ysize)\n",
    "    transform = src.window_transform(window)\n",
    "\n",
    "    # Create a new cropped raster to write to\n",
    "    profile = src.profile\n",
    "    profile.update({\n",
    "        'height': xsize,\n",
    "        'width': ysize,\n",
    "        'transform': transform})\n",
    "\n",
    "    with rasterio.open('D:/haar/classes/carros_output.tif', 'w', **profile) as dst:\n",
    "        # Read the data from the window and write it to the output raster\n",
    "        dst.write(src.read(window=window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "shape_o=gpd.read_file(r\"D:\\alexnet\\carros.shp\")\n",
    "shape_o=shape_o.to_crs('3857')\n",
    "# shape=shape_transform(shape_o.copy())\n",
    "shape_aumentado=ampliar_shape(shape_o.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_aumentado.to_file(r\"D:\\alexnet\\carros1.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import fiona\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import mapping,Polygon\n",
    "src_raster_path = r\"C:\\Users\\ASUS\\Desktop\\presentacionyolov7\\etiquetas_google1\\neza.tif\"\n",
    "src=rasterio.open(src_raster_path)\n",
    "H,W=src.shape\n",
    "terrenos=gpd.read_file(r\"C:\\Users\\ASUS\\Desktop\\presentacionyolov7\\vero 2\\vero\\vero1.shp\")\n",
    "terrenos=terrenos.set_crs(3857)\n",
    "terrenos=terrenos.to_crs(3857)\n",
    "terrenos=ampliar_shape(terrenos,factor_ampliacion=1.25)\n",
    "import tqdm\n",
    "import cv2\n",
    "for i,polygo in tqdm.tqdm(enumerate(terrenos[\"geometry\"]),total=len(terrenos)):\n",
    "    det=\"a\"\n",
    "    try:\n",
    "        shapes=[mapping(polygo)]\n",
    "    except:\n",
    "        continue\n",
    "    out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "    array, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "    four_images=[array[2],array[1],array[0]]\n",
    "    stacked_images = np.stack(four_images, axis=-1)\n",
    "    cv2.imwrite(r\"D:\\alexnet\\train/\"+terrenos.loc[i,\"clase_dete\"]+\"4anew_\"+str(i)+\".png\",stacked_images)\n",
    "    # cv2.imshow(\"s\",stacked_images)\n",
    "    # k=cv2.waitKey(0) & 0xFF\n",
    "    # if k==ord(\"a\"):\n",
    "    #     det=\"casa_\"\n",
    "    # elif k==ord(\"s\"):\n",
    "    #     det=\"carros_\"\n",
    "    # elif k==ord(\"d\"):\n",
    "    #     det=\"establecimiento_\"\n",
    "    # elif k==ord(\"f\"):\n",
    "    #     det=\"multivivienda_\"\n",
    "    # elif k==ord(\"g\"):\n",
    "    #     det=\"encontruccion_\"\n",
    "    # elif k==ord(\"h\"):\n",
    "    #     det=\"terreno_\"\n",
    "    # elif k<28:\n",
    "    #     cv2.destroyAllWindows()\n",
    "    #     break\n",
    "    cv2.imwrite(r\"D:\\alexnet\\train/\"+terrenos.loc[i,\"clase_dete\"]+\"4anew_\"+str(i)+\".png\",stacked_images)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ampliar_shape(shape,factor_ampliacion=2):\n",
    "    \"\"\"Amplifica el polygon de cada manzana con el fin de extrar imagenes sin perder informacion de la manzana\"\"\"\n",
    "    shape[\"geometry\"]=shape[\"geometry\"]\n",
    "    shape['centroid']=shape.centroid\n",
    "    geometry=[]\n",
    "    clase=[]\n",
    "    for i,polygon in enumerate(shape['geometry']):\n",
    "        try:\n",
    "            point=mapping(shape['centroid'][i]).get('coordinates')\n",
    "        except:\n",
    "            continue\n",
    "        x=point[0]\n",
    "        y=point[1]\n",
    "        go=[]\n",
    "        coodinates=mapping(polygon).get('coordinates')[0]\n",
    "        for a in coodinates:\n",
    "            x1=a[0]\n",
    "            y1=a[1]\n",
    "            x2=x+(x1-x)*factor_ampliacion\n",
    "            y2=y+(y1-y)*factor_ampliacion\n",
    "            go.append((x2,y2))\n",
    "        geometry.append(Polygon(go))\n",
    "        clase.append(shape.loc[i,\"clase_dete\"])\n",
    "    return gpd.GeoDataFrame({\"clase_dete\":clase},geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
